{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-Nandam/TensorFlow-Notebooks/blob/main/02_neural_network_classification_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqNgME2lu3pN"
      },
      "source": [
        "# Introduction to neural network classification with TensorFlow\n",
        "\n",
        "In this notebook, we're going to learn how to write neural networks for classification problems.\n",
        "\n",
        "A classification is when you try to classify something as one thing or another.\n",
        "\n",
        "A few types of classification problems:\n",
        "* Binary classification\n",
        "* Multiclass classfication\n",
        "* Multilabel classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0y8UTavQrZ"
      },
      "source": [
        "## Creating data to view and fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yc8nDo_U0zL_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Make 1000 examples\n",
        "n_samples = 1000\n",
        "\n",
        "# Create circles\n",
        "X, y = make_circles(n_samples, noise=0.03, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCQLNPdD1qLO"
      },
      "outputs": [],
      "source": [
        "# Check out the features\n",
        "X, X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZZiwwWd2KKi"
      },
      "outputs": [],
      "source": [
        "# Check the labels\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR9Vizj52hdQ"
      },
      "outputs": [],
      "source": [
        "np.count_nonzero(y==1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data is a little hard to understand right now... let's visualize it!"
      ],
      "metadata": {
        "id": "0NjjQMw33PKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "circles = pd.DataFrame({\"X0\": X[:, 0], \"X1\": X[:, 1], \"label\": y})\n",
        "circles"
      ],
      "metadata": {
        "id": "DfZY2sgE67Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circles[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "XfQ_t0c7P8X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize with a plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)"
      ],
      "metadata": {
        "id": "bj9FHqoj7J-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ›  **Exercise:** Before pushing forward, spend 10-minutes playing ariund with [playground.tensorflow.org](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.08000&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) building and running different neural networks. See what happens when you change different hyperparameters."
      ],
      "metadata": {
        "id": "Hr3QbHQI7iuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input and Output shapes"
      ],
      "metadata": {
        "id": "y8e3r8rqKGEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of our features and labels\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "nuVKcuF-KbEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many samples we are working with\n",
        "len(X), len(y)"
      ],
      "metadata": {
        "id": "p43uLL2eKgBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first example of features and labels\n",
        "X[0], y[0]"
      ],
      "metadata": {
        "id": "R6q0rPdiKhVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps in modelling\n",
        "\n",
        "The steps in modelling with TensorFlow are typically:\n",
        "\n",
        "1. Create or import a model\n",
        "2. Compile the model\n",
        "3. Fit the model\n",
        "4. Evaluate the model\n",
        "5. Tweak \n",
        "6. Evaluate..."
      ],
      "metadata": {
        "id": "JtAvYvGkKrSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "iSfaHnuzN435"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model using the Sequential API\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.SGD(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_1.fit(X, y, epochs=5)"
      ],
      "metadata": {
        "id": "3AxAyZplK0kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try and improve our model by training for longer...\n",
        "model_1.fit(X, y, epochs=200, verbose=0)\n",
        "model_1.evaluate(X, y)"
      ],
      "metadata": {
        "id": "cleLmCSpOIzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're working on a binary classification problem and our model is getting around ~50% accuracy... it's performing as if it's guessing.\n",
        "\n",
        "So let's step things up a notch and add an extra layer."
      ],
      "metadata": {
        "id": "x3M3FDhlOxRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1),\n",
        "    tf.keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.SGD(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_2.fit(X, y, epochs=100, verbose=0)"
      ],
      "metadata": {
        "id": "QB-z5BSrO0Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Evaluate the model\n",
        "model_2.evaluate(X, y)"
      ],
      "metadata": {
        "id": "sGrbsvqmPnLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circles[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "EyNqHtuTPxBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving our model\n",
        "\n",
        "Let's look into our bag of tricks to see how we can improve our model.\n",
        "\n",
        "1. Create a model - we might want to add more layers or increase the number of hidden units within a layer.\n",
        "2. Compiling a model - here we might want to choose a different optimization function such as Adam instead of SGD.\n",
        "3. Fitting a model - perhaps we might want our model to train for longer."
      ],
      "metadata": {
        "id": "mZhDcr6jQxEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model (this time 3 layers)\n",
        "model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),     # add 100 dense neurons\n",
        "    tf.keras.layers.Dense(10),      # add another layer with 10 neurons\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_3.fit(X, y, epochs=100, verbose=0)"
      ],
      "metadata": {
        "id": "aO2r5tTGRffQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Evaluate the model\n",
        "model_3.evaluate(X, y)"
      ],
      "metadata": {
        "id": "KN6a0VYmS1d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.predict(X)"
      ],
      "metadata": {
        "id": "ngV13amPS6N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”‘ **Note:** Whenever your model is performing strangely or there's something going on with your data you're not quite sure of, remember these three words: **visualize, visualize, visualize**. Inspect your data, inspect your model, inpsect your model's predictions.\n",
        "\n",
        "To visualize our model's predictions, let's create a function `plot_decision_boundary()`, this function will:\n",
        "\n",
        "* Take in a trained model, features (X) and labels (y).\n",
        "* Create a meshgrid of the different X values.\n",
        "* Make predictions across the meshgrid.\n",
        "* Plot the predictions as well as a line between zones (where each unique class falls)"
      ],
      "metadata": {
        "id": "D-lCWpkmT7OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    \"\"\"\n",
        "    Plot the decision boundary created by a model predicting on X.\n",
        "    This function was inspired by 2 resources:\n",
        "        1. CS231n - https://cs231n.github.io/neural-networks-case-study/\n",
        "        2. Made with ML basics - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\n",
        "    \"\"\"\n",
        "    # Define the axis boundaries of the plot and create a meshgrid\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "    \n",
        "    # Create X values (we're going to make predictions on these)\n",
        "    x_in = np.c_[xx.ravel(), yy.ravel()]    # stack 2D arrays together\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(x_in)\n",
        "\n",
        "    # Check for multiclass\n",
        "    if len(y_pred[0]) > 1:\n",
        "        print(\"Doing multiclass classification\")\n",
        "        # We have to reshape our prediction to get them ready for plotting\n",
        "        y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n",
        "    else:\n",
        "        print(\"Doing binary classification\")\n",
        "        y_pred = np.round(y_pred).reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())"
      ],
      "metadata": {
        "id": "zMSPMiAQUmGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the predictions our model is making\n",
        "plot_decision_boundary(model=model_3, X=X, y=y)"
      ],
      "metadata": {
        "id": "lPDxoJkHWnNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see of our model can be used for a regression problem...\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create some regression data\n",
        "X_regression = tf.range(0, 1000, 5)\n",
        "y_regression = tf.range(100, 1100, 5)   # y = X + 100\n",
        "\n",
        "# Split our regression data into training and test sets\n",
        "X_reg_train = X_regression[:150]\n",
        "X_reg_test = X_regression[150:]\n",
        "y_reg_train = y_regression[:150]\n",
        "y_reg_test = y_regression[150:]\n",
        "\n",
        "# Fit our model to the regression data\n",
        "model_3.fit(X_reg_train, y_reg_train, epochs=100)"
      ],
      "metadata": {
        "id": "OCuTyXXwXPLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh wait... we compiled our model for a binary classification problem.\n",
        "\n",
        "But... we're working on a regression problem, let's change the model to suit our data."
      ],
      "metadata": {
        "id": "EjInf6zkbLpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "# 2. Compile the model, this time with regression specific loss function\n",
        "model_3.compile(loss=tf.keras.losses.mae,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_3.fit(tf.expand_dims(X_reg_train, axis=-1), y_reg_train, epochs=100)"
      ],
      "metadata": {
        "id": "WP7_6pSlZwV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with our trained model\n",
        "y_reg_preds = model_3.predict(X_reg_test)\n",
        "\n",
        "# Plot the model's predictions against our regression data\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(X_reg_train, y_reg_train, c=\"b\", label=\"Training data\")\n",
        "plt.scatter(X_reg_test, y_reg_test, c=\"g\", label=\"Test data\")\n",
        "plt.scatter(X_reg_test, y_reg_preds, c=\"r\", label=\"Predictions\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "WaPKaZaldNFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The missing piece: Non-linearity"
      ],
      "metadata": {
        "id": "xfL5irM0eFII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_4 = tf.keras.Sequential([\n",
        "    # tf.keras.layers.Dense(1, activation=\"linear\")\n",
        "    tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "hisotry = model_4.fit(X, y, epochs=100)"
      ],
      "metadata": {
        "id": "kRwwbeUxg6d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out our data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)"
      ],
      "metadata": {
        "id": "H6KHuHNEjVnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the decision boundary for our latest model\n",
        "plot_decision_boundary(model=model_4, X=X, y=y)"
      ],
      "metadata": {
        "id": "9mtbWOSQj--X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try build our first neural netwrok with non-linear activation function."
      ],
      "metadata": {
        "id": "_cVQyirwkvna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create a model with a non-linear activation\n",
        "model_5 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation=tf.keras.activations.relu)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_5.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "history = model_5.fit(X, y, epochs=100)"
      ],
      "metadata": {
        "id": "qNzHHdXakIV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Time to replicate the muli-layer neural network from TensorFlow playground\n",
        "\n",
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_6 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_6.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_6.fit(X, y, epochs=250)"
      ],
      "metadata": {
        "id": "1myv3wNTlelb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model_6.evaluate(X, y)"
      ],
      "metadata": {
        "id": "1zTm6qEDpqq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do our model predictions look?\n",
        "plot_decision_boundary(model_6, X, y)"
      ],
      "metadata": {
        "id": "FLRCVRI-pwz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_7 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_7.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_7.fit(X, y, epochs=100)"
      ],
      "metadata": {
        "id": "oXw-7_qUqMUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our model\n",
        "model_7.evaluate(X, y)"
      ],
      "metadata": {
        "id": "Vt5MhGzWr_wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize our incredible metrics\n",
        "plot_decision_boundary(model_7, X, y)"
      ],
      "metadata": {
        "id": "mATd_6mCrZO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¤” **Question:** What's wrong with predictions we've made? Are we really evaluating our model correclty? Hint: What data did the model learn on and what data did we predict on?\n",
        "\n",
        "ðŸ”‘ **Note:** The combination of **linear (straight lines) and non-linear (non-straight lines) functions** is one of the key fundamentals of neural networks.\n",
        "\n",
        "Now we've discussed the concept of linear and non-linear function (or lines), let's see them in action"
      ],
      "metadata": {
        "id": "ia7xptXCrb3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a toy tensor (similar to the data we pass into our models)\n",
        "A = tf.cast(tf.range(-10, 10), tf.float32)\n",
        "A"
      ],
      "metadata": {
        "id": "wm3V73ZsskVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our toy tensor\n",
        "plt.plot(A);"
      ],
      "metadata": {
        "id": "gsjvvl9YLsVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start by replicating sigmoid - sigmoid(x) = 1 / (1 + exp(-x))\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + tf.exp(-x))\n",
        "\n",
        "# Use the sigmoid function on our toy tensor\n",
        "sigmoid(A)"
      ],
      "metadata": {
        "id": "jpmL6AgbM1oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot out toy tensor transformed by sigmoid\n",
        "plt.plot(sigmoid(A));"
      ],
      "metadata": {
        "id": "DV_3g0cbOMss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's recreate the relu function\n",
        "def relu(x):\n",
        "    return tf.maximum(0, x)\n",
        "\n",
        "# Pass our toy tensor to our custom relu function\n",
        "relu(A)"
      ],
      "metadata": {
        "id": "abVFcLlsOS13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot relu modified tensor\n",
        "plt.plot(relu(A));"
      ],
      "metadata": {
        "id": "KeGrutYWO9Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try the linear activation function\n",
        "tf.keras.activations.linear(A)"
      ],
      "metadata": {
        "id": "7fu8W2R6PK8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Does the linear activation function change anything?\n",
        "plt.plot(tf.keras.activations.linear(A))"
      ],
      "metadata": {
        "id": "SETbFttlPsQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Does A even change?\n",
        "A == tf.keras.activations.linear(A)"
      ],
      "metadata": {
        "id": "x4hXhPFzP1iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating and improving our classification\n",
        "\n",
        "So far we've been training and testing on the same dataset...\n",
        "\n",
        "However, in Machine Learning this is basically a sin.\n",
        "\n",
        "So, let's create a training and test set."
      ],
      "metadata": {
        "id": "8j4Z0AjgP6aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many examples we have\n",
        "len(X)"
      ],
      "metadata": {
        "id": "l_Ewgpt4Q3vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test sets\n",
        "X_train, y_train = X[:800], y[:800]\n",
        "X_test, y_test = X[800:], y[800:]\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "UOLamZagRfG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's recreate a model to find on the training data and evaluate on the test data.\n",
        "\n",
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model (same as model_7)\n",
        "model_8 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_8.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "history = model_8.fit(X_train, y_train, epochs=25)"
      ],
      "metadata": {
        "id": "xEl1eq72RwfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Evaluate the model on the test dataset\n",
        "model_8.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "SH76SmhfTW2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the decision boundaries for the training and test sets\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_8, X=X_train, y=y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_8, X=X_test, y=y_test)"
      ],
      "metadata": {
        "id": "kcllr9GyTquK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the loss (or training) curves"
      ],
      "metadata": {
        "id": "s-ycPuwVUC6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the history object into a DataFrame\n",
        "pd.DataFrame(history.history)"
      ],
      "metadata": {
        "id": "Rj99qYX1UlYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "pd.DataFrame(history.history).plot()\n",
        "plt.title(\"Model_8 loss curves\");"
      ],
      "metadata": {
        "id": "Imp-LKquVE-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”‘ **Note:** For many problems, the loss function going down means the model is improving (the predictions it's making are getting closer to the ground truth labels)."
      ],
      "metadata": {
        "id": "WuaFyWppVa_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the best learning rate\n",
        "\n",
        "To find the ideal learning rate (the learning rate where the loss decreases the most during training), we're going to use the following steps:\n",
        "* A learning rate **callback*** - you can think of a callback as an extra piece of functionality, you can add to your model *while it's training.*\n",
        "* Another model (we could use the same one as above, but we're practicing building models here)\n",
        "* A modified loss curves plot."
      ],
      "metadata": {
        "id": "wPFb-xcTVvxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create a model (same as model_8)\n",
        "model_9 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_9.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Create a learning rate callback\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10 ** (epoch / 20))\n",
        "\n",
        "# 3. Fit the model (passing lr_scheduler callback)\n",
        "history_9 = model_9.fit(X_train, y_train, epochs=100, callbacks=[lr_scheduler])"
      ],
      "metadata": {
        "id": "oLjRs0JPXkS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkout the history\n",
        "pd.DataFrame(history_9.history).plot(figsize=(10, 7), xlabel=\"epochs\");"
      ],
      "metadata": {
        "id": "nKC23TJ0ZB1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the learning rate versus the loss\n",
        "lrs = 1e-4 * (10 ** (tf.range(100) / 20))\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.semilogx(lrs, history_9.history[\"loss\"])\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Learning rate vs. Loss\");"
      ],
      "metadata": {
        "id": "iGIfNommZfsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of other typical learning rates values:\n",
        "10 ** 0, 10 ** -1, 10 ** -2, 10 ** -3, 1e-4"
      ],
      "metadata": {
        "id": "U4UQ-30qaC_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate we used before\n",
        "10 ** -2"
      ],
      "metadata": {
        "id": "RVc1Gv9idMGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try using a higher *ideal* learning rate with the same model\n",
        "\n",
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_10 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# 2. Compile the model with the ideal learning rate\n",
        "model_10.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.02),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model for 20 epochs (5 less than before)\n",
        "history_10 = model_10.fit(X_train, y_train, epochs=20)"
      ],
      "metadata": {
        "id": "yGcgOqbYaDmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model_10 on the test dataset\n",
        "model_10.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "BbZrvh5TdrSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model_8 on the test dataset\n",
        "model_8.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "gqeUcrmBeCvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the decision boundaries for the training and test sets\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_10, X=X_train, y=y_train)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_10, X=X_test, y=y_test)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "tFpuDj1OeJ8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More classification evaluation metrics\n",
        "\n",
        "Alongside visualizing our model's results as much as possible, there are a handful of other classification evaluation methods & metrics you should be familiar with:\n",
        "\n",
        "* Accuracy\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-Score\n",
        "* Confusion matrix\n",
        "* Classification report (from SciKit-Learn)"
      ],
      "metadata": {
        "id": "AMSHWt8Seoof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the accuracy of our model\n",
        "loss, accuracy = model_10.evaluate(X_test, y_test)\n",
        "print(f\"Model loss on the test set: {loss}\")\n",
        "print(f\"Model accuracy on the test set: {(accuracy * 100):.2f}%\")"
      ],
      "metadata": {
        "id": "H9sc7LVHzGag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How about a confusion matrix?"
      ],
      "metadata": {
        "id": "A2C6LTMYzi7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Make predictions\n",
        "y_preds = model_10.predict(X_test)\n",
        "\n",
        "# Create confusion matrix\n",
        "confusion_matrix(y_test, y_preds)"
      ],
      "metadata": {
        "id": "Ob4bhCZrzrVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:10]"
      ],
      "metadata": {
        "id": "bm75JyX00Vs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds[:10]"
      ],
      "metadata": {
        "id": "dLPzTHdD0fBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oops... looks like our predictions array has come out in **prediction probability** form... the standard output from the sigmoid (or softmax) activation functions."
      ],
      "metadata": {
        "id": "7pdHI59p0hCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert prediction probabilities to binary format and view the first 10\n",
        "tf.math.round(y_preds)[:10]"
      ],
      "metadata": {
        "id": "YrdiG3Lq1Azw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix\n",
        "confusion_matrix(y_test, tf.math.round(y_preds))"
      ],
      "metadata": {
        "id": "d-kET1sg1LJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How about we pretiify our confusion matrix?"
      ],
      "metadata": {
        "id": "Aon0f_uT1mne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: the confusion matix code we're abput to write is a remix of scikit-learn's plot_confusion_matrix\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix\n",
        "\n",
        "import itertools\n",
        "\n",
        "figsize = (10, 10)\n",
        "\n",
        "# Create the confusion matrix\n",
        "cm = confusion_matrix(y_test, tf.math.round(y_preds))\n",
        "cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize our confusion matrix\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "# Let's prettify it\n",
        "fig, ax = plt.subplots(figsize=figsize)\n",
        "# Create a matrix plot\n",
        "cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
        "fig.colorbar(cax)\n",
        "\n",
        "# Create classes\n",
        "classes = False\n",
        "\n",
        "if classes:\n",
        "    labels = classes\n",
        "else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "\n",
        "# Label the axes\n",
        "ax.set(title=\"Confusion Matrix\",\n",
        "       xlabel=\"Predicted Label\",\n",
        "       ylabel=\"True Label\",\n",
        "       xticks=np.arange(n_classes),\n",
        "       yticks=np.arange(n_classes),\n",
        "       xticklabels=labels,\n",
        "       yticklabels=labels)\n",
        "\n",
        "# Set x-axis labels to bottom\n",
        "ax.xaxis.set_label_position(\"bottom\")\n",
        "ax.xaxis.tick_bottom()\n",
        "\n",
        "# Adjust label size\n",
        "ax.yaxis.label.set_size(20)\n",
        "ax.xaxis.label.set_size(20)\n",
        "ax.title.set_size(20)\n",
        "\n",
        "# Set the threshold for different colors\n",
        "threshold = (cm.max() + cm.min()) / 2\n",
        "\n",
        "# Plot the text on each cell\n",
        "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j] * 100:.1f}%)\",\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "             size=15)"
      ],
      "metadata": {
        "id": "psF_vtkMBh24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with a larger example (multiclass classification)\n",
        "\n",
        "When you have more than two classes as an option, it's known as **multi-class classification**.\n",
        "\n",
        "* This means if you have 3 different classes, it's multi-class classification.\n",
        "* It also means if you have 100 different classes, it's multi-class classification.\n",
        "\n",
        "To practice multi-class classification, we're going to build a neural network to classify images of different items of clothing."
      ],
      "metadata": {
        "id": "I-2xTc9oChI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# The data has already been sorted into training and test sets for us\n",
        "(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "ALkUVwxgIB0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first training example\n",
        "print(f\"Training sample:\\n{train_data[0]}\\n\")\n",
        "print(f\"Training label:\\n{train_labels[0]}\\n\")"
      ],
      "metadata": {
        "id": "A1YXRsAHJrd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of a single example\n",
        "train_data[0].shape, train_labels[0].shape"
      ],
      "metadata": {
        "id": "U2TTAOAuJ6h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a single sample\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(train_data[7]);"
      ],
      "metadata": {
        "id": "rLlM1FllKW_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out sample's label\n",
        "train_labels[7]"
      ],
      "metadata": {
        "id": "hD5usPbKKmRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small list so we can index onto our training labels so they're human readable\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "len(class_names)"
      ],
      "metadata": {
        "id": "OHpCQQu7KulV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot an example image and its label\n",
        "index_of_choice = 17\n",
        "plt.imshow(train_data[index_of_choice], cmap=plt.cm.binary)\n",
        "plt.title(class_names[train_labels[index_of_choice]]);"
      ],
      "metadata": {
        "id": "_RsBj1_wWeWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot multiple random images of fashion MNIST\n",
        "import random\n",
        "plt.figure(figsize=(7, 7))\n",
        "for i in range(4):\n",
        "    ax = plt.subplot(2, 2, i + 1)\n",
        "    rand_index = random.choice(range(len(train_data)))\n",
        "    plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n",
        "    plt.title(class_names[train_labels[rand_index]])\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "3QEFrPiQXWIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a multi-class classification model\n",
        "\n",
        "For our multi-class classification model, we can use a similar architecture to our binary classifiers, however, we're going to have to tweak a few things:\n",
        "* Input shape = 28 x 28 (the shape of one image)\n",
        "* Output shape = 10 (one per class of clothing)\n",
        "* Loss function = tf.keras.losses.CategoricalCrossentropy()\n",
        "    * If your labels are one-hot encoded, use CategoricalCrossentropy()\n",
        "    * If your labels are integer form use SparseCategoricalCrossentropy()\n",
        "* Output layer activation = Softmax (not Sigmoid)"
      ],
      "metadata": {
        "id": "SMZ596W1dG9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.one_hot(train_labels[:10], depth=10)"
      ],
      "metadata": {
        "id": "vHi46d7AhKQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_11 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,  28)), # our data needs to flattened from (28 * 28) to (None, 784)\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_11.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                 optimizer=tf.keras.optimizers.Adam(),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "non_norm_history = model_11.fit(train_data, \n",
        "                                tf.one_hot(train_labels, depth=10),\n",
        "                                epochs=10,\n",
        "                                validation_data=(test_data, tf.one_hot(test_labels, depth=10)))"
      ],
      "metadata": {
        "id": "0j6-urT1dM6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model summary\n",
        "model_11.summary()"
      ],
      "metadata": {
        "id": "8ce0bUNdgw7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the min and max values of the training data\n",
        "train_data.min(), train_data.max()"
      ],
      "metadata": {
        "id": "LpERCGXuk9eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks prefer data to be scaled (or normalized), this means they like to have the numbers in the tensors they try to find patterns between 0 & 1."
      ],
      "metadata": {
        "id": "yiAYMqmQpFuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can get our training and testing data between 0 & 1 by dividing by the maximum\n",
        "train_data_norm = train_data / 255.0\n",
        "test_data_norm = test_data / 255.0\n",
        "\n",
        "# We can check the min and max values of the scaled training data\n",
        "train_data_norm.min(), train_data_norm.max()"
      ],
      "metadata": {
        "id": "kyEwZNh1pask"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now our data is normalized. let's build a model to find patterns in it\n",
        "\n",
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_12 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,  28)), # our data needs to flattened from (28 * 28) to (None, 784)\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_12.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                 optimizer=tf.keras.optimizers.Adam(),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "norm_history = model_12.fit(train_data_norm, \n",
        "                                train_labels,\n",
        "                                epochs=10,\n",
        "                                validation_data=(test_data_norm, test_labels))"
      ],
      "metadata": {
        "id": "l3Lzpl6Spzsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ”‘ **Note:** Neural networks tend to prefer data in numerical form as well as scaled/normalized (numbers between 0 & 1)."
      ],
      "metadata": {
        "id": "ENMP7JxJp7o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Plot non-normalized data loss curves\n",
        "pd.DataFrame(non_norm_history.history).plot(title=\"Non-normalized data\");\n",
        "# Plot normalized data loss curves\n",
        "pd.DataFrame(norm_history.history).plot(title=\"Normalized data\");"
      ],
      "metadata": {
        "id": "WLQbRMlmrrAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ”‘ **Note:** The same model with even *slightly* different data can produce *dramatically* different results. So when you're comparing models, it's important to make sure you're comparing them on the same criteria (e.g. same architecture but different data or same data but different architecture)."
      ],
      "metadata": {
        "id": "jrCHQW8bvKfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the ideal learning rate"
      ],
      "metadata": {
        "id": "rUsoS0WivvFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_13 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_13.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                 optimizer=tf.keras.optimizers.Adam(),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# Create the learning rate callback\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))\n",
        "\n",
        "# 3. Fit the model\n",
        "find_lr_history = model_13.fit(train_data_norm,\n",
        "                               train_labels,\n",
        "                               epochs=40,\n",
        "                               validation_data=(test_data_norm, test_labels),\n",
        "                               callbacks=[lr_scheduler])"
      ],
      "metadata": {
        "id": "CrDt7ANw0_4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the learning rate decay curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lrs = 1e-3 * (10**(tf.range(40)/20))\n",
        "plt.semilogx(lrs, find_lr_history.history[\"loss\"])\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Finding the ideal learning rate\");"
      ],
      "metadata": {
        "id": "mSUGtgYe1L9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10 ** -3"
      ],
      "metadata": {
        "id": "VB246DV692ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's refit a model with the ideal learning rate\n",
        "\n",
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create the model\n",
        "model_14 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(4, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model_14.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history_14 = model_14.fit(train_data_norm,\n",
        "                          train_labels,\n",
        "                          epochs=20,\n",
        "                          validation_data=(test_data_norm, test_labels))"
      ],
      "metadata": {
        "id": "Dndelm_p97w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating our multi-class classification model\n",
        "\n",
        "To evaluate our multi-class classificarion model we could:\n",
        "* Evaluate it's performance using other classification metrics (such as a confusion matrix)\n",
        "* Assess some of it's predictions (through visualizations)\n",
        "* Improve it's results (by training it for longer or changing the architecture)\n",
        "* Save and export it for use in an application\n",
        "\n",
        "Let's go through the top 2..."
      ],
      "metadata": {
        "id": "RXHFKTk7_2Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15):\n",
        "\n",
        "    # Create the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_preds)\n",
        "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize our confusion matrix\n",
        "    n_classes = cm.shape[0]\n",
        "\n",
        "    # Let's prettify it\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    # Create a matrix plot\n",
        "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set labels to be classes\n",
        "    if classes:\n",
        "        labels = classes\n",
        "    else:\n",
        "        labels = np.arange(cm.shape[0])\n",
        "\n",
        "    # Label the axes\n",
        "    ax.set(title=\"Confusion Matrix\",\n",
        "        xlabel=\"Predicted Label\",\n",
        "        ylabel=\"True Label\",\n",
        "        xticks=np.arange(n_classes),\n",
        "        yticks=np.arange(n_classes),\n",
        "        xticklabels=labels,\n",
        "        yticklabels=labels)\n",
        "\n",
        "    # Set x-axis labels to bottom\n",
        "    ax.xaxis.set_label_position(\"bottom\")\n",
        "    ax.xaxis.tick_bottom()\n",
        "\n",
        "    # Adjust label size\n",
        "    ax.yaxis.label.set_size(text_size)\n",
        "    ax.xaxis.label.set_size(text_size)\n",
        "    ax.title.set_size(text_size)\n",
        "\n",
        "    # Set the threshold for different colors\n",
        "    threshold = (cm.max() + cm.min()) / 2\n",
        "\n",
        "    # Plot the text on each cell\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j] * 100:.1f}%)\",\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "                 size=text_size)"
      ],
      "metadata": {
        "id": "BIWmPcVfCW4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "id": "vVoNLVEBF7WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some predictions with our model\n",
        "y_probs = model_14.predict(test_data_norm) # probs is short for \"prediction probabilities\"\n",
        "\n",
        "# View the first 5 predictions\n",
        "y_probs[:5]"
      ],
      "metadata": {
        "id": "Umnj6vjkGCR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ”‘ **Note:** Remember to make predictions on the same kind of data your model was trained on (e.g. if your model was trained on normalized data, you'll want to make predictions on normalized data)."
      ],
      "metadata": {
        "id": "geU71CBgF-Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_probs[0], tf.argmax(y_probs[0]), tf.math.reduce_sum(y_probs[0])"
      ],
      "metadata": {
        "id": "0ot-18eUHNbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all of the prediction probabilities into integers\n",
        "y_preds = y_probs.argmax(axis=1)\n",
        "\n",
        "# View the first 10 prediction labels\n",
        "y_preds[:10]"
      ],
      "metadata": {
        "id": "hIV2FLtBHOoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_true=test_labels,\n",
        "                 y_pred=y_preds)"
      ],
      "metadata": {
        "id": "0G9YH-ddHQRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prettir confusion matrix\n",
        "make_confusion_matrix(y_true=test_labels,\n",
        "                      y_pred=y_preds,\n",
        "                      classes=class_names,\n",
        "                      figsize=(15, 15),\n",
        "                      text_size=10)"
      ],
      "metadata": {
        "id": "GgyG7Ls_R8Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ”‘ **Note:** Often when working with images and other forms of visual data, it's a good idea to visualize as much as possible to develop a further understanding of the data and the inputs and outputs of your models.\n",
        "\n",
        "How about we create a fun little function for:\n",
        "* Plot a random image\n",
        "* Make a prediction on said image\n",
        "* Label the plot with truth label & the predicted label"
      ],
      "metadata": {
        "id": "iiDAwj4hSb4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def plot_random_image(model, images, true_labels, classes):\n",
        "    '''\n",
        "    Picks a random image, plots it and lables it with a prediction and truth label.\n",
        "    '''\n",
        "    # Set up random integer\n",
        "    i = random.randint(0, len(images))\n",
        "\n",
        "    # Create predictions and targets\n",
        "    target_image = images[i]\n",
        "    pred_probs = model.predict(target_image.reshape(1, 28, 28))\n",
        "    pred_label = classes[pred_probs.argmax()]\n",
        "    true_label = classes[true_labels[i]]\n",
        "\n",
        "    # Plot the image\n",
        "    plt.imshow(target_image, cmap=plt.cm.binary)\n",
        "\n",
        "    # Change the color of the titles depending on if the prediction is right or wrong\n",
        "    if pred_label == true_label:\n",
        "        color = \"green\"\n",
        "    else:\n",
        "        color = \"red\"\n",
        "\n",
        "    # Add xlabel information (prediction/true label)\n",
        "    plt.xlabel(\"Pred: {} {:.2f}% (True: {})\".format(pred_label,\n",
        "                                                    100 * tf.reduce_max(pred_probs),\n",
        "                                                    true_label),\n",
        "               color=color) # set the color to green or red based on if prediction is right or wrong\n"
      ],
      "metadata": {
        "id": "zFAwwwpDT0S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out a random image as well as its prediction\n",
        "plot_random_image(model=model_14,\n",
        "                  images=test_data_norm, # always make predictions on the same kind of data your model was trained on\n",
        "                  true_labels=test_labels,\n",
        "                  classes=class_names)"
      ],
      "metadata": {
        "id": "Y33bzqA4Uy16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What patterns is our model learning?"
      ],
      "metadata": {
        "id": "3ETCrmYWWznt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the layers of our most recent model\n",
        "model_14.layers"
      ],
      "metadata": {
        "id": "gSQffAtrd888"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract a particular layer \n",
        "model_14.layers[1]"
      ],
      "metadata": {
        "id": "c_P2L-mHeVGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the patterns of a layer in our network\n",
        "weights, biases = model_14.layers[1].get_weights()\n",
        "\n",
        "# Shapes\n",
        "weights, weights.shape"
      ],
      "metadata": {
        "id": "Uf2XgpOged9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_14.summary()"
      ],
      "metadata": {
        "id": "Zcyv_g8VerhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's check out the bias vector..."
      ],
      "metadata": {
        "id": "6y4DQp1Ce4zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bias and Biases shapes\n",
        "biases, biases.shape"
      ],
      "metadata": {
        "id": "OuqsPM4DxarF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every neuron has a bias vector. Each of these is paired with a weights matrix.\n",
        "\n",
        "The bias vector get initialized as zeros (at least in the case of a TensorFlow Dense layer).\n",
        "\n",
        "The bias vector dictates how much the patterns within the corresponding weights matrix should influence the next layer."
      ],
      "metadata": {
        "id": "yI-NtSq7xfJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_14.summary()"
      ],
      "metadata": {
        "id": "7jRi8HMcyMQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out another way of viewing our deep learning models\n",
        "from tensorflow.keras.utils import plot_model\n",
        "# See the inputs and outputs of each layer\n",
        "plot_model(model_14, show_shapes=True)"
      ],
      "metadata": {
        "id": "D7CDF_zqyzYg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "02-neural-network-classification-with-tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyODIdUIc/Gbv6S/eRwUJ3tq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}