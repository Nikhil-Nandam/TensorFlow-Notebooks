{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-neural-network-regression-with-tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFJPrOcveXrqpv0s1y+Uy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhil-Nandam/TensorFlow-Notebooks/blob/main/01_neural_network_regression_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Regression with Neural Networks in TensorFlow\n",
        "\n",
        "There are many definitions for a regression problem but in our case, we're going to simplify it: predicting a numerical variable based on some other combination of variables, even shorter... predicting a number."
      ],
      "metadata": {
        "id": "9DU-F61bExxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "o1fEp3-UofNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating data to view and fit"
      ],
      "metadata": {
        "id": "sX4B2qoSopR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create features \n",
        "X = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n",
        "\n",
        "# Create labels\n",
        "y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n",
        "\n",
        "# Visualize it\n",
        "plt.scatter(X, y)"
      ],
      "metadata": {
        "id": "Se_bPjDIoziI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X + 10"
      ],
      "metadata": {
        "id": "CvwBgztNpWru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y == X + 10"
      ],
      "metadata": {
        "id": "iqaop4oUplyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input and output shapes"
      ],
      "metadata": {
        "id": "1mi9j-g6ppp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a demo tensor for our housing price prediction problem\n",
        "house_info = tf.constant(['bedroom', 'bathroom', 'garage'])\n",
        "house_price = tf.constant([939700])\n",
        "house_info, house_price"
      ],
      "metadata": {
        "id": "OCK4ivhppsy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0], y[0]"
      ],
      "metadata": {
        "id": "MVS1vmTZqrbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[1], y[1]"
      ],
      "metadata": {
        "id": "A02LfpgnquB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X[0].shape\n",
        "output_shape = y[0].shape\n",
        "input_shape, output_shape"
      ],
      "metadata": {
        "id": "HAvK2cWcqau6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].ndim"
      ],
      "metadata": {
        "id": "bTslkRGBqmtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0], y[0]"
      ],
      "metadata": {
        "id": "weJcM3oTrA8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our NumPy arrays into tensors with dtype float32\n",
        "X = tf.cast(tf.constant(X), dtype=tf.float32)\n",
        "y = tf.cast(tf.constant(y), dtype=tf.float32)\n",
        "X, y"
      ],
      "metadata": {
        "id": "WZVrMIXJrGZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = X[0].shape\n",
        "output_shape = y[0].shape\n",
        "input_shape, output_shape"
      ],
      "metadata": {
        "id": "DGrnV9P7rhaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X, y)"
      ],
      "metadata": {
        "id": "-TtuzB1Crk73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps in modelling with TensorFlow\n",
        "\n",
        "1. **Creating a model** - define the input and output layers, as well as the hidden layers of a deep learning model.\n",
        "2. **Compiling a model** - define the loss function (in other words, the function whicht tells our model how wrong it is) and the optimizer (tell our model how to improve the patterns its learning) and evaluation metrics (what we can use to interpret the performance of our model).\n",
        "3. **Fitting a model** - letting the model try to find patterns between X & y (features and labels)."
      ],
      "metadata": {
        "id": "Y7WBpt03rvJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create a model using the Sequential API\n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss=tf.keras.losses.mae, # mae is shot for mean absolute error\n",
        "              optimizer=tf.keras.optimizers.SGD(), # sgd is short for stochastic gradient descent\n",
        "              metrics=['mae'])\n",
        "\n",
        "# 3. Fit the model\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)"
      ],
      "metadata": {
        "id": "ii-P_9svry48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out X and y\n",
        "X, y"
      ],
      "metadata": {
        "id": "wnvXYsCcv0hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try and make a prediction using our model\n",
        "y_pred = model.predict([17.0])\n",
        "y_pred"
      ],
      "metadata": {
        "id": "cbiIapQkwwZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred + 11"
      ],
      "metadata": {
        "id": "kMnxT6iixD_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving our model\n",
        "\n",
        "We can improve our model, by altering the steps we took to create a model.\n",
        "\n",
        "1. **Creating a model** - Here we might add more layers, increase the number of hidden units (also called neurons) within each of the hidden layers, change the activation function of each layer.\n",
        "2. **Compiling a model** - Here we might change the optimization function or perhaps the **learning rate** of the optimization function.\n",
        "3. **Fitting a model** - Here we might fit a model for more **epochs** (leave it training for longer) or on more data (give the model more examples to learn from)."
      ],
      "metadata": {
        "id": "2pRZBlKQxUue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's rebuild our model\n",
        "\n",
        "# 1. Create a model using the Sequential API\n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss=tf.keras.losses.mae, # mae is shot for mean absolute error\n",
        "              optimizer=tf.keras.optimizers.SGD(), # sgd is short for stochastic gradient descent\n",
        "              metrics=['mae'])\n",
        "\n",
        "# 3. Fit the model (this time we'll train for longer)\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=100)"
      ],
      "metadata": {
        "id": "xeAOH-o4xdm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remind ourselves of the data\n",
        "X, y"
      ],
      "metadata": {
        "id": "ThE2ObQ62XRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if our model's prediction has improved\n",
        "y_pred = model.predict([17.])\n",
        "y_pred"
      ],
      "metadata": {
        "id": "MLXJT7j22une"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's rebuild our model again\n",
        "\n",
        "# 1. Create a model using the Sequential API\n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(50, activation=None),\n",
        "        tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss=tf.keras.losses.mae, # mae is shot for mean absolute error\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # sgd is short for stochastic gradient descent\n",
        "              metrics=['mae'])\n",
        "\n",
        "# 3. Fit the model (this time we'll train for longer)\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=100)"
      ],
      "metadata": {
        "id": "YM9p3vwn26HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if our model's prediction has improved\n",
        "y_pred = model.predict([17.0])\n",
        "y_pred"
      ],
      "metadata": {
        "id": "03aGWUd93N0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”‘ **Note:** The learning rate is the most important hyper-parameter to improve our model."
      ],
      "metadata": {
        "id": "rt10qrYn3XHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating a model\n",
        "\n",
        "In practice, a typical workflow you'll go through when building neural networks is:\n",
        "\n",
        "```\n",
        "Build a model -> fit it -> evaluate it -> tweak a model -> fit it -> evaluate it -> tweak a model -> fit it -> evaluate it ...\n",
        "```"
      ],
      "metadata": {
        "id": "6Wbf2KRn7Z11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it comes to evaluation... there a 3 words you should memorize:\n",
        "\n",
        "> \"Visualize, visualize, visualize\"\n",
        "\n",
        "It's a good idea to visualize:\n",
        "* The data - what data are we working with? What does it look like?\n",
        "* The model itself - what does our model look like?\n",
        "* The training of a model - how does a model perform while it learns?\n",
        "* The predictions of the model - how do the predictions of the model line up against the ground truth (the original labels)?"
      ],
      "metadata": {
        "id": "R5sopWGjEr77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Make a bigger dataset\n",
        "X = tf.range(-100, 100, 4)\n",
        "X"
      ],
      "metadata": {
        "id": "gMod3sSIGUKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make labels for the dataset\n",
        "y = X + 10\n",
        "y"
      ],
      "metadata": {
        "id": "HHeB0Ec9GaT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the data\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X, y)"
      ],
      "metadata": {
        "id": "PGmX8QzIGjLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The 3 sets...\n",
        "\n",
        "* **Training set** - the model learns from this data, which typically 70-80% of the total data available.\n",
        "* **Validation set** - the model gets tuned on this data, which is typically 10-15% of the data available data.\n",
        "* **Test set** - the model gets evaluated in this data to test what it has learned, this set is typically 10-15% of the total data available."
      ],
      "metadata": {
        "id": "nUWhXsyrGs1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Check the length of how many samples we have\n",
        "len(X)"
      ],
      "metadata": {
        "id": "VNXTNHX5WbnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "X_train = X[:40] # first 40 training samples (80% of the data)\n",
        "y_train = y[:40]\n",
        "\n",
        "X_test = X[40:] # last 10 training sample (20% of the data)\n",
        "y_test = y[40:]\n",
        "\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "Ejnd2HA_XE_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the data\n",
        "\n",
        "Now we've got out data in training and test sets... let's visualize it again!"
      ],
      "metadata": {
        "id": "xIIPaBNuXtEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "# Plot training data in blue\n",
        "plt.scatter(X_train, y_train, c='b', label='Training data')\n",
        "# Plot test data in green\n",
        "plt.scatter(X_test, y_test, c='g', label='Testing data')\n",
        "# Show a legend\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "VQVk_hpNX7tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's have a look at how to build a neural network for our data\n",
        "\n",
        "# 1. Create a model\n",
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.SGD(),\n",
        "              metrics=['mae'])\n",
        "\n",
        "# 3. Fit the model\n",
        "# model.fit(X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "id": "yJQ5gNdlYUT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the model"
      ],
      "metadata": {
        "id": "-tTfJ05_ZnHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "baIUboTRZvwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0], y[0]"
      ],
      "metadata": {
        "id": "sqMbfU6HafbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a model which builds automatically by defining the input_shape argument in the first layer\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create a model (same as above)\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(10, input_shape=[1], name='input_layer'),\n",
        "                             tf.keras.layers.Dense(1, name='output_layer')\n",
        "], name='model_1')\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.SGD(),\n",
        "              metrics=['mae'])"
      ],
      "metadata": {
        "id": "65HcXWkqZxDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "LOWGqPpEa1J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Total params - total number of parameter in the model.\n",
        "* Trainable params - these are the parameters (patterns) the model can update as it trains.\n",
        "* Non-trainable params - these parameters aren't updated during training (this is typical when you bring in already learnt patterns or parameters from other models during **transfer learning**).\n",
        "\n",
        "ðŸ“– **Resource:** For a more in-depth overview of thr trainable parameters within a layer, check out MIT's introduction to deep learning video.\n",
        "\n",
        "ðŸ›  **Exercise:** Try playing around with the number of hidden units in the dense layer, see how that affects the number of parameters (total and trainable) by calling `model.summary()`."
      ],
      "metadata": {
        "id": "gMQUYvF1a2Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's fit the model to the training data\n",
        "model.fit(X_train, y_train, epochs=100, verbose=0)"
      ],
      "metadata": {
        "id": "8vwnBmbKb2nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of our model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3raQVRRUd5p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model=model, show_shapes=True)"
      ],
      "metadata": {
        "id": "G_JitJdAejXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize our model's predictions\n",
        "\n",
        "To visualize predictions, it's a goog idea to plot them against the ground truth labels.\n",
        "\n",
        "Often you'll see this in the form of `y_test` or `y_true` versus `y_pred` (ground truth versus your model's predictions)."
      ],
      "metadata": {
        "id": "trvGpAI2fNmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "tdB-MXUWifI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "Dq4FAR_SinK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”‘ **Note:** If you feel like you're going to reuse some kind of functionality in the future, it's a good idea to turn it into a function."
      ],
      "metadata": {
        "id": "4ix6XjZ9i2Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a plotting function\n",
        "def plot_predictions(train_data=X_train, \n",
        "                     train_labels=y_train,\n",
        "                     test_data=X_test,\n",
        "                     test_labels=y_test,\n",
        "                     predictions=y_pred):\n",
        "    \"\"\"\n",
        "    Plots training data, test data and compares predictions to ground truth labels.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    # Plot training data in blue\n",
        "    plt.scatter(train_data, train_labels, c='b', label='Training data')\n",
        "    # Plot testing data in green\n",
        "    plt.scatter(test_data, test_labels, c='g', label='Testing data')\n",
        "    # Plot model's predictions in red\n",
        "    plt.scatter(test_data, predictions, c='r', label='Predictions')\n",
        "    # Show the legend\n",
        "    plt.legend();"
      ],
      "metadata": {
        "id": "zN9lPx9sio4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(train_data=X_train, \n",
        "                 train_labels=y_train,\n",
        "                 test_data=X_test,\n",
        "                 test_labels=y_test,\n",
        "                 predictions=y_pred\n",
        "                 )"
      ],
      "metadata": {
        "id": "tu8uWyCckUkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating our model's prediction with regression evaluation metrics\n",
        "\n",
        "Depending on the problem you're working on, there will be different evaluation metrics to evaluate your model's performance.\n",
        "\n",
        "Since we're working on a regression, two of the main metrics:\n",
        "* MAE - mean abolute error, \"on average, how wrong is each of my model's predictions\"\n",
        "* MSE - mean squared error, \"sqaure the average errors\""
      ],
      "metadata": {
        "id": "iFtLGge2kX61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "wDCwkGO6k0h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "qrNd-6wymt-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "ae8N6m2HnOro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.squeeze(y_pred)"
      ],
      "metadata": {
        "id": "WHXbcK5_pOwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean absolute error\n",
        "mae = tf.keras.losses.MeanAbsoluteError()\n",
        "mae(y_test, tf.squeeze(y_pred)).numpy()"
      ],
      "metadata": {
        "id": "xt5N81PtnPcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.losses.mean_absolute_error(y_test, tf.squeeze(y_pred)).numpy()"
      ],
      "metadata": {
        "id": "SUGoEb8enn0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean squared error\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "mse(y_test, tf.squeeze(y_pred)).numpy()"
      ],
      "metadata": {
        "id": "hBV5ld8AohMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.metrics.mean_squared_error(y_test, tf.squeeze(y_pred)).numpy()"
      ],
      "metadata": {
        "id": "OTpuQzkrp3V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some functions to reuse MAE and MSE\n",
        "def mae(y_true, y_pred):\n",
        "    return tf.keras.metrics.mean_absolute_error(y_true, tf.squeeze(y_pred))\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return tf.keras.metrics.mean_squared_error(y_true,tf.squeeze(y_pred))"
      ],
      "metadata": {
        "id": "uHhxR7mnqA2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running experiments to improve our model\n",
        "\n",
        "\n",
        "```\n",
        "Build a model -> fit it -> evaluate it -> tweak it -> fit it -> evaluate it -> tweak it -> fit it -> evaluate it ...\n",
        "```\n",
        "\n",
        "1. Get nore data - get more examples for your model to train on (more oppurtunities to learn patterns or relationships between features and labels).\n",
        "2. Make your model larger (using a more complex model) - this might come in the form of more layers or more hidden units in each layer.\n",
        "3. Train for longer - give your mmodel of a chance to find patterns in the data.\n",
        "\n",
        "Let's do 3 modelling experiments:\n",
        "\n",
        "1. `model_1` - same as the original model, 1 layer, trained for 100 epochs.\n",
        "2. `model_2` - 2 layers, trained for 100 epochs.\n",
        "3. `model_3` - 2 layers, trained for 500 epochs.\n",
        "\n",
        "**Build `model_1`**"
      ],
      "metadata": {
        "id": "HwLrecnHq3-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train"
      ],
      "metadata": {
        "id": "U_y31IgMEFpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_1.compile(loss=tf.keras.losses.mae,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['mae'])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_1.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100)"
      ],
      "metadata": {
        "id": "a9CRfBc9AzHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make and plot the predictions for model_1\n",
        "y_preds_1 = model_1.predict(X_test)\n",
        "plot_predictions(predictions=y_preds_1)"
      ],
      "metadata": {
        "id": "df1ViiT7Bl6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_1 evaluation metrics\n",
        "mae_1 = mae(y_test, y_preds_1)\n",
        "mse_1 = mse(y_test, y_preds_1)\n",
        "mae_1, mse_1"
      ],
      "metadata": {
        "id": "s1znEjhlFga9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build `model_2`**\n",
        "\n",
        "* 2 dense layers, trained for 100 epochs"
      ],
      "metadata": {
        "id": "DJTJsXQuGm_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_2.compile(loss=tf.keras.losses.mae,\n",
        "                optimizer=tf.keras.optimizers.SGD(),\n",
        "                metrics=['mse'])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_2.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100)"
      ],
      "metadata": {
        "id": "BBx6vg5oHaaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make an plot predictions of model_2\n",
        "y_preds_2 = model_2.predict(X_test)\n",
        "plot_predictions(predictions=y_preds_2)"
      ],
      "metadata": {
        "id": "nnGjmnjiJLUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_2 evaluation metrics\n",
        "mae_2 = mae(y_test, y_preds_2)\n",
        "mse_2 = mse(y_test, y_preds_2)\n",
        "mae_2, mse_2"
      ],
      "metadata": {
        "id": "lA9V-tXnJc4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build `model_3`**\n",
        "\n",
        "* 2 layers, trained for 500 epochs"
      ],
      "metadata": {
        "id": "RW5kce5xJtPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_3.compile(loss=tf.keras.losses.mae,\n",
        "                optimizer=tf.keras.optimizers.SGD(),\n",
        "                metrics=['mse'])\n",
        "\n",
        "# 3. Fit the model\n",
        "model_3.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=500)"
      ],
      "metadata": {
        "id": "KpUF_DviKI-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make our predictions\n",
        "y_preds_3 = model_3.predict(X_test)\n",
        "plot_predictions(predictions=y_preds_3)"
      ],
      "metadata": {
        "id": "9Ke7Tqz7KJiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics\n",
        "mae_3 = mae(y_test, y_preds_3)\n",
        "mse_3 = mse(y_test, y_preds_3)\n",
        "mae_3, mse_3"
      ],
      "metadata": {
        "id": "GKeN9ZTcKfyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”‘ **Note:** You want to start with small experiments (small models) and make sure they work and then increase their scale when necessary."
      ],
      "metadata": {
        "id": "Iu7u2FGKL906"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing the results of our experiments\n",
        "\n",
        "We've run a few experiments, let's compare our results."
      ],
      "metadata": {
        "id": "eNyR58bAKzm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare our model's results using a pandas DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "model_results = [['model_1', mae_1.numpy(), mse_1.numpy()],\n",
        "                 ['model_2', mae_2.numpy(), mse_2.numpy()],\n",
        "                 ['model_3', mae_3.numpy(), mse_3.numpy()]]\n",
        "\n",
        "all_results = pd.DataFrame(model_results, columns=['model', 'mae', 'mse'])\n",
        "all_results"
      ],
      "metadata": {
        "id": "4fuLb2juLVcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like `model_2` performed the best..."
      ],
      "metadata": {
        "id": "pI16MIgFNc7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "A8_fN0RRNIZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”‘ **Note:** One of your main goals should be to minimize the time between your experiments. The more experiments you do, the more things you'll figure out which don't work and in turn, get closer to figuring out what does work. Remember machine learning practioner's motto: \"experiment, experiment, experiment\"."
      ],
      "metadata": {
        "id": "a2kbyiYcNQbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tracking your experiments\n",
        "\n",
        "One really good habit in machine learning modelling is to track the results of your experiments.\n",
        "\n",
        "And when doing so, it can be tedious if you're running lots of experiments.\n",
        "\n",
        "Luckily, there are tools to help us!\n",
        "\n",
        "ðŸ“– **Resource:** As you build more models, you'll want to look into using:\n",
        "\n",
        "* TensorBoard - a component of the TensorFlow library to help track modelling experiments (we'll see this one later).\n",
        "* Weights & Biases - a tool for tracking all kinds of machine learning experiments (plugs straight into TensorBoard)."
      ],
      "metadata": {
        "id": "S_bURq4NOHJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving our models\n",
        "\n",
        "Saving our models allows us to use them outside of Google Colab (or whever they were trained) such as in a web application or a mobile application.\n",
        "\n",
        "There are two main formats we can save our model's too:\n",
        "1. SavedModel format\n",
        "2. The HDF5 format"
      ],
      "metadata": {
        "id": "ee0hyQuTPXt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model using the SavedModel format\n",
        "model_2.save(\"best_model_SavedModel_format\")"
      ],
      "metadata": {
        "id": "o0Nj_7dCnFPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model using the HDF5 format\n",
        "model_2.save(\"best_model_HDF5_format.h5\")"
      ],
      "metadata": {
        "id": "GX1p_XAgor4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading in a saved model"
      ],
      "metadata": {
        "id": "VP4gMjrZpiXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the SavedModel format model\n",
        "loaded_SavedModel_format = tf.keras.models.load_model(\"best_model_SavedModel_format\")\n",
        "loaded_SavedModel_format.summary()"
      ],
      "metadata": {
        "id": "VqkBnG2KqBBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "h51WK2r9qZJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare model_2 predictions with SavedModel format model predictions\n",
        "model_2_preds = model_2.predict(X_test)\n",
        "loaded_SavedModel_format_preds = loaded_SavedModel_format.predict(X_test)\n",
        "model_2_preds == loaded_SavedModel_format_preds"
      ],
      "metadata": {
        "id": "P7rPlvfAqcmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in a model using the .h5 format\n",
        "loaded_h5_model = tf.keras.models.load_model(\"best_model_HDF5_format.h5\")\n",
        "loaded_h5_model.summary()"
      ],
      "metadata": {
        "id": "829ryrp6q4si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "dnjD3pNurjyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to if loaded .h5 predictions match model_2\n",
        "model_2_preds = model_2.predict(X_test)\n",
        "loaded_h5_model_preds = loaded_h5_model.predict(X_test)\n",
        "model_2_preds == loaded_h5_model_preds"
      ],
      "metadata": {
        "id": "RsVpGD3Orlqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download a model (or any other file) from Google Colab\n",
        "\n",
        "If you want to download your files from Google Colab:\n",
        "\n",
        "1. You can go to the \"files\" tab and right click on the file you're after and click \"download\".\n",
        "2. Use code (see the cell below).\n",
        "3. Save it Google Drive by connecting Google Drive and copying it there (see 2nd code cell below)."
      ],
      "metadata": {
        "id": "JRoFf-elr732"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a file from Google Colab\n",
        "from google.colab import files\n",
        "files.download(\"best_model_HDF5_format.h5\")"
      ],
      "metadata": {
        "id": "DNcu6NkVsjHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A larger example"
      ],
      "metadata": {
        "id": "5wGc5h5-s8U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "2YHSCMmux8tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the insurance dataset\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n",
        "insurance"
      ],
      "metadata": {
        "id": "efqvv4fQzHeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance[\"smoker\"], insurance[\"age\"]"
      ],
      "metadata": {
        "id": "FeQtmbzgz375"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try one-hot encode our DataFrame so it's all numbers\n",
        "insurance_one_hot = pd.get_dummies(insurance)\n",
        "insurance_one_hot.head()"
      ],
      "metadata": {
        "id": "L6icF_Eg05O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create X & y values\n",
        "X = insurance_one_hot.drop(\"charges\", axis=1)\n",
        "y = insurance_one_hot[\"charges\"]"
      ],
      "metadata": {
        "id": "YjzJhu8o2QwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View X\n",
        "X.head()"
      ],
      "metadata": {
        "id": "QqL5pIxr-2Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View y\n",
        "y.head()"
      ],
      "metadata": {
        "id": "4Bpkt8-F-6bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test_sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "len(X), len(X_train), len(X_test)"
      ],
      "metadata": {
        "id": "qayb7I9s98PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "IiowLSiA_88K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a neural network (sort of like model_2 above) \n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create a model\n",
        "insurance_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10), \n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "insurance_model.compile(loss=tf.keras.losses.mae,\n",
        "                        optimizer=tf.keras.optimizers.SGD(),\n",
        "                        metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "insurance_model.fit(X_train, y_train, epochs=100)"
      ],
      "metadata": {
        "id": "79wVWhwe-lnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the results of the insurance model of the test data\n",
        "insurance_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "MzWrsbe_A-ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.median(), y_train.mean()"
      ],
      "metadata": {
        "id": "04eam6A3BHO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now it looks like our model isn't performing too well... let's try and improve it!\n",
        "\n",
        "To (try) improve our model, we'll run 2 experiments:\n",
        "1. Add an extra layer with more hidden units and use the Adam optimizer.\n",
        "2. Same as above but train for longer (200 epochs).\n",
        "3. (insert your own experiment here).\n"
      ],
      "metadata": {
        "id": "0xUKdVNEBXhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train"
      ],
      "metadata": {
        "id": "XobV0EeR5QVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "insurance_model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "insurance_model_2.compile(loss=tf.keras.losses.mae,\n",
        "                          optimizer=tf.keras.optimizers.Adam(),\n",
        "                          metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "insurance_model_2.fit(X_train, y_train, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "id": "vsqquQZo30Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_model_2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "NNG5M7G15Ara"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "3ZWeYJEX5KI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "insurance_model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "insurance_model_3.compile(loss=tf.keras.losses.mae,\n",
        "                          optimizer=tf.keras.optimizers.Adam(),\n",
        "                          metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "history = insurance_model_3.fit(X_train, y_train, epochs=200, verbose=1)"
      ],
      "metadata": {
        "id": "w7KlECq-5uEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our third model\n",
        "insurance_model_3.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "QvyDC7z86ucv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "XzEmVetJ693k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot history (also known as a loss curve or a training curve)\n",
        "pd.DataFrame(history.history).plot()\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epochs\")"
      ],
      "metadata": {
        "id": "EGkcaDtI7Dgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¤” **Question:** How long should you train for?\n",
        "\n",
        "It depends. Really... it depends on the problem you're working on. However, many people have asked this question before... so TensorFlow has a solution! It's called the EarlyStopping Callback, which is a TensorFlow component you can add to your model to stop training once it stops improving a certian metric."
      ],
      "metadata": {
        "id": "Yw6nDoaO7csu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data (normalisation and standardization)\n",
        "\n",
        "In terms of scaling values, neural networks tend to prefer normalization.\n",
        "\n",
        "If you're not sure which to use, you could try both and see which performs better."
      ],
      "metadata": {
        "id": "d0Kid1dJ9v5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Read in the insurance dataframe\n",
        "insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n",
        "insurance"
      ],
      "metadata": {
        "id": "n567tdaBTVdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare our data, we can borrow a few classes from SciKit-Learn."
      ],
      "metadata": {
        "id": "V05ZX3lvUALb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a column transformer\n",
        "ct = make_column_transformer(\n",
        "    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]),      # turn all values in these columns between 0 and 1.\n",
        "    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n",
        ")\n",
        "\n",
        "# Create X & y\n",
        "X = insurance.drop(\"charges\", axis=1)\n",
        "y = insurance[\"charges\"]\n",
        "\n",
        "# Build our train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the column transformer to our training data\n",
        "ct.fit(X_train)\n",
        "\n",
        "# Transform training and test data with normalization (MinMaxScaler) and OneHotEncoder\n",
        "X_train_normal = ct.transform(X_train)\n",
        "X_test_normal = ct.transform(X_test)"
      ],
      "metadata": {
        "id": "J5CKQ5B-RBED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does our data look like now?\n",
        "X_train.loc[0], X_train_normal[0]"
      ],
      "metadata": {
        "id": "XnOg0d7eWV8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_train_normal.shape"
      ],
      "metadata": {
        "id": "gRuaQKFZWgP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beautiful! Our data has been normalized and one hot encoded. Now let's build a neural network model on it and see how it goes."
      ],
      "metadata": {
        "id": "kin-XVMnWsYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a neural network model to fit on our normalized data\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Create the model\n",
        "insurance_model_4 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# 2.Compile the model\n",
        "insurance_model_4.compile(loss=tf.keras.losses.mae,\n",
        "                          optimizer=tf.keras.optimizers.Adam(),\n",
        "                          metrics=[\"mae\"])\n",
        "\n",
        "# 3. Fit the model\n",
        "insurance_model_4.fit(X_train_normal, y_train, epochs=100)"
      ],
      "metadata": {
        "id": "7m1CxXJIXJVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our insurance model trained on normalized data\n",
        "insurance_model_4.evaluate(X_test_normal, y_test)"
      ],
      "metadata": {
        "id": "_1xK4ferYS0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c405tD98YeVe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}